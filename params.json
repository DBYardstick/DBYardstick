{"name":"DBYardstick","tagline":"","body":"\r\nOpen-source database benchmark implementations, using less resources.\r\n\r\nElevator Pitch\r\n==============\r\n\r\nThis project aims to implement the database benchmarks to a T, so much so that\r\nthe results obtained from these tests can pass an auditor's scrutiny and can be\r\npublished as official results.\r\n\r\nAgreed that there's a lot more to a benchmark than the code to test the SUT\r\n(system under test), but the architecture, design, quality and implementation of\r\nthe test driver code determines how many resources one has to spend on the\r\nclient-side hardware to run the benchmarks.\r\n\r\nIf the client-side hardware requirements are reduced, the benchmark sponsor\r\ncan spend more on the hardware for the System Under Test (SUT, a.k.a the\r\n'database' being tested).\r\n\r\nMission Statement\r\n=================\r\n\r\nDevelop open-source implementations of database benchmarks that stick to the\r\nspecifications, allow for the variations permitted by the specification, and,\r\nmost importantly, be very light on the hardware so that running a benchmark\r\ndoesn't cost someone an arm and a leg.\r\n\r\nStrategic Planning\r\n==================\r\n\r\nSee the \"Decisions\" section below.\r\n\r\nProject Status\r\n==============\r\n\r\nThe TPC-C benchmark has been implemented. It is currently capable of running\r\n100,000 clients against an infinitely fast database (implemented as `NullDB` in\r\ncode), while consuming just one CPU.\r\n\r\nThe code is beta quality as of now (meaning, no known bugs but there's scope for\r\nenhancements).\r\n\r\nThe architecture of this implementation allows one to easily port this benchmark\r\napplication to another database, without any changes to the application itself.\r\nTo test a new database system, implement a new interface to that database\r\nand develop the database specific transaction implementations; see `postgres_db.ts`\r\nfor an example.\r\n\r\nDecisions\r\n=========\r\n\r\nThe following decisions are driven by the need to implement a highly scalable\r\nimplementation of TPC-C benchmark.\r\n\r\nI had been thinking of implementing this benchmark for a couple of years, and I\r\nwas convinced that NodeJS' ability to allow easily and cheaply create\r\nasynchronous, highly concurrent network applications was the right way to\r\nimplement the benchmark.\r\n\r\nSo I spent a day researching technology options and reading through  the TPC-C\r\nbenchmark (my first target benchmark). I considered following:\r\n\r\n0. One process (or thread) per terminal is not an option\r\n\r\n\tIt appears, from some of the past 'Full Disclosure' reports I read, that the\r\nnorm is to create an application that simulates an order-entry terminal, and\r\nrun multiple instances of it from multiple computers. With a maximum tpmC at\r\n1.2 per terminal ([Max tpmC Slide]) (because of the percentages of New Order\r\ntransactions and keying/think times), one computer simply cannot host enough\r\nprocesses for the tests to deliver decent tpmC. For eg. with the default maximum\r\nprocess limit of around 65,000 in Linux, we would get 78,000 tpmC, given that the\r\nOS doesn't actually come to a grinding halt by the time it creates 65k processes.\r\n\r\n\tBy requiring many physical computers to run the TPC-C tests, it would make it\r\nprohibitively expensive for smaller organisations to even try this test the way\r\nthe benchmark specification mandates.\r\n\r\n[Max tpmC Slide]: http://www.tpc.org/information/sessions/sigmod/sld016.htm\r\n\r\n1. Node.js application\r\n\r\n\tFirst I thought of implementing the benchmark as a node.js application, such\r\nthat a number of asynchronous clients would be hitting the Postgres server\r\nasynchronously, and I can call it a day.\r\n\r\n\tBut reading the TPC-C benchmark description, it appears that terminals are\r\nquite important a requirement of the benchmark. I guess the auditor has to be\r\nable to see the application/terminal in action. So this idea was dropped.\r\n\r\n\tBy this time I had reviewed all unit-tests of node-postgres package, and was\r\nconvinced that this should be used. I did see some deficiencies in the tests and\r\ncode, but they can be improved. Eg. developed a patch for the tests; and the\r\n'readyForQuery' message doesn't know the current transaction state, even though\r\nthe Postgres wire protocol version 3.0 conveys that, and there's no built-in\r\nsupport for composing transactions, except holding on the the connection\r\nforcefully.\r\n\r\n2. Implementing project using node-postgres-pure, inside browser.\r\n\r\n\tThis would require that the pg.js library be made to work over WebSockets,\r\nand make Postgres understand WebSockets. I went back and read code of WSLAY\r\nproject (I have contributed some code to that project in the past). I also read\r\nthe message-layout/data-framing protocol of WebSockets (section 5.2 of RFC 6455).\r\n\r\n\tIdea was sound, but as is evident, too much work is involved in making\r\ncurrent projects talk over WebSockets. WebSockets itself is not free, in terms\r\nof performance; apart from the startup-overhead of HTTP Upgrade handshake, every\r\nmessage has to be encoded/decoded to extract the data that is otherwise sent raw\r\nin the TCP communication. Also, even if I modified Postgres to understand\r\nWebSockets, later when we'd have to introduce a connection pooler (TP monitor,\r\nin TPC-C parlance), we'd have to modify that connection pooler as well to\r\nunderstand WebSockets.\r\n\r\n3. Back to square one: application in Node.js\r\n\r\n\tIt dawned on me, why not use libpq's asynchronous API to talk to Postgres\r\nusing many connections from a single process. So I investigated that angle. But\r\nto display a user-interface (UI) I would have to resort to one of the likes of\r\ncurses/ncurses libraries. Although using those libraries doesn't seem very hard,\r\nmanaging multiple asynchronous PG connections, and tying the interface of any\r\none of those connections to the UI at will, feels hard. I would like to have the\r\nauditor choose any one of the many terminals at random, and see how transactions\r\nare progressing on that terminal; this combined with the fact that terminals are\r\nsupposed to account for keying times and thinking times, so I'd have to put many\r\nclients to sleep for different deterministic periods, which is no easy feat if\r\ndone in C language.\r\n\r\n\tI remembered in some recent NodeJS articles that there are now libraries to\r\nwrite terminal programs. Research brought up 'blessed' and I think it is the right\r\ntool for the job (only time will tell :). It has quite sophisticated UI controls,\r\nand it'd allow for easy tying of a model (order-entry terminal) to a view (the\r\nconsole) in asynchronous fashion.\r\n\r\n4. TypeScript\r\n\r\n\tLast but not the least, the choice of language. I understand JavaScript to\r\ncertain extent, just enough to make heads and tails of others' code, but I am\r\nquite sure it will get pretty ugly pretty soon, and it will be difficult, if not\r\nimpossible, to prove to the auditor that the code does what the benchmark\r\nrequires.\r\n\r\n\tSo I have chosen TypeScript as the language, even though I haven't coded\r\nanything outside of its playground application. But I have great confidence in\r\nits ability to bring clarity to a complex, weakly-typed world of JavaScript.\r\n\r\n5. No AngularJS\r\n\r\n\tAfter I had decided upon using NodeJS and Blessed to develop the tests, it\r\noccurred to me why not use AngularJS Framework (or something similar) to make it\r\neasier for model-view binding; that is, just make a change in the model, and the\r\nframework will take care of updating the view (terminal in our case).\r\n\r\n\tI love the concept of AngularJS because it makes it very easy to separate\r\nthe models from the view, and works almost like magic. But this magic comes at a\r\ncost. After a little more thought, I decided against using AngularJS\r\nspecifically, because it seems to me that it compares the old and new values of\r\nthe objects to decide if something has changed in the model, which means that it\r\nhas to keep copies of objects. Since this magic requires extra memory footprint\r\nand more CPU execution, it would incur increased latencies immediately in the\r\nbenchmark driver, and that isn't acceptable, especially when the work we'd have\r\nto do in the absence of this framework is minimal.\r\n\r\n6. License\r\n\r\n\tI chose GPL v3 License because I agree with its philosophy. I don't want\r\nTivoization to limit this code being run on someone's hardware. If you own the\r\nhardware, you should be able to run this code on it, with whatever modifications\r\nyou deem fit.\r\n\r\n\tAs of now I believe that as the owner of this project, I can change my mind\r\nlater to adopt GPLv2, if I later disagree with some of the attributes of GPLv3.\r\nOf course, the code that was previously distributed under GPLv3 will remain\r\nunder GPLv3.\r\n\r\n7. Do not pass JSON formatted strings to and from the database\r\n\r\n\tIn Postgres, the `to_json()` and its counterpart `json_populate_record()` can\r\nbe used to marshal and unmarshal the JSON data. So it is possible to send JSON\r\nstrings to the database and make it send the result as JSON as well. That would\r\nsimplify the TPC-C application-side logic a bit because the application is being\r\ndeveloped in JavaScript here.\r\n\r\n  But I chose to not use JSON for database communication purely for performance\r\nreasons. Making the database do something that can be done by the client would\r\nmake database performance suffer.\r\n\r\n8. Do not pass user-defined types to and from the database\r\n\r\n\tIn the same vein as not passing JSON back and forth, do not pass objects\r\neither. The serialization and deserialization of user-defined types consumes\r\nCPU which can be put to better use.\r\n\r\n  TODO: The code currently in place violates both the above decisions, for lack\r\nof time. Fix code to adhere to the above two decisions.\r\n\r\n9. Use `Repeatable Read` as the default transaction isolation level\r\n\r\n  This is to satisfy the requirements of the TPC-C specification, which requires\r\nthat the `New Order`, `Payment`, `Delivery` and `Order Status` transactions\r\nshould have `repeatable read` isolation from each other (see clause 3.4 of the\r\nspecification for more details). Also, the 'Stock Level' transactions are subject\r\nto less stringent isolation: `read committed`.\r\n\r\n\tA transaction operating with `repeatable read` isolation may encounter\r\nserialization errors. But since 'Order Status' and 'Stock Level' transactions are\r\nread-only transactions, they will never encounter these serialization errors in\r\nPostgres. So I don't bother changing transaction isolation to `read committed`\r\nbefore executing `Stock Level` transaction; not doing so reduces at least one\r\nnetwork round-trip per `Stock Level` transaction.\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}